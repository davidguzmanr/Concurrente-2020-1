{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyCUDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrLbHz6Je7js",
        "colab_type": "text"
      },
      "source": [
        "# CUDA y PyCUDA\n",
        "\n",
        "Google Colab ya viene con CUDA preinstalado, lo cual lo podemos checar con lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyv86yigkOgM",
        "colab_type": "code",
        "outputId": "84a34d09-bcfe-4851-baa4-8427f1d84e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "b!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvw_o49tkUz2",
        "colab_type": "text"
      },
      "source": [
        "Sin embargo, no tiene PyCUDA, por lo que debemos instalarlo (tardará un par de minutos):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ana70sfNF5Xf",
        "colab_type": "code",
        "outputId": "549f1a61-8cc8-4d1b-8b59-dc08a51a8afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "!curl https://colab.chainer.org/install | sh -\n",
        "!pip install pycuda\n",
        "!pip install scikit-cuda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0  21944      0 --:--:-- --:--:-- --:--:-- 21944\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "+ set +ex\n",
            "Installation succeeded!\n",
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.6/dist-packages (2019.1.2)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (1.4.3)\n",
            "Requirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.1)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.6/dist-packages (from pycuda) (1.1.0)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.6/dist-packages (from pycuda) (2019.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.17.4)\n",
            "Requirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.12.0)\n",
            "Requirement already satisfied: scikit-cuda in /usr/local/lib/python3.6/dist-packages (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-cuda) (1.17.4)\n",
            "Requirement already satisfied: pycuda>=2016.1 in /usr/local/lib/python3.6/dist-packages (from scikit-cuda) (2019.1.2)\n",
            "Requirement already satisfied: mako>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-cuda) (1.1.0)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pycuda>=2016.1->scikit-cuda) (1.4.3)\n",
            "Requirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda>=2016.1->scikit-cuda) (4.4.1)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.6/dist-packages (from pycuda>=2016.1->scikit-cuda) (2019.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako>=1.0.1->scikit-cuda) (1.1.1)\n",
            "Requirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda>=2016.1->scikit-cuda) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tQzpTnm6lKBJ"
      },
      "source": [
        "# 1. CUDA\n",
        "\n",
        "## 1.1 ¿Por qué un GPU?\n",
        "\n",
        "La principal problemática con un CPU hoy en día, y la razón por la que tenemos que pensar en paralelo es que estamos llegando a un límite de velocidad de las arquitecturas actuales. Esto es debido a que para que tenga una mayor velocidad un procesador, hay que utilizar más potencia en este. Después de cierto punto, se vuelve ineficiente o imposible mantener el procesador lo suficientemente frío como para operar correctamente.\n",
        "\n",
        "Por otro lado, visualicemos cuantos hilos se pueden ejecutar en paralelo en un CPU. Ejemplo: \n",
        "\n",
        "\n",
        "\n",
        "*   Procesador Intel Skylake (2017).\n",
        "*   Velocidad de 3.5 GHz base.\n",
        "*   Tiene ocho núcleos físicos.\n",
        "*   Puede realizar operaciones de vectores de 512 bits, es decir, 16 operaciones simultáneas con números de 32 bits. (AVX)\n",
        "*   Hyperthreading, básicamente divide el núcleo físico y lo hace ver como dos núcleos lógicos, lo que deja realizar aproximadamente dos hilos al mismo tiempo.\n",
        "*    Esto nos lleva a un máximo teórico de $\n",
        "2\\cdot16\\cdot8 = 256$ hilos paralelos.\n",
        "\n",
        "Por otro lado:\n",
        "\n",
        "\n",
        "\n",
        "*   Nvidia GTX 980 (2014).\n",
        "*   Velocidad de 1.127 GHz base.\n",
        "*   Posee 2048 unidades de cómputo para sombreado.\n",
        "*   Esto nos lleva a 2048 hilos en paralelo sólo de unidades de sombreado.\n",
        "\n",
        "Como podemos notar, incluso varias generaciones atrás, las GPUs podían realizar una mayor cantidad de operaciones concurrentes que una CPU normal, y no están atadas tan fuertemente por las limitaciones de un CPU (correlación directa entre velocidad y nivel de paralelismo, así como velocidad y calor). Hoy en día, esta diferencia se ha incrementado enormemente.\n",
        "\n",
        "## 1.2 Paradigmas de la programación de GPU\n",
        "\n",
        "Aunque es claro que se pueden realizar muchas más operaciones concurrentes en un GPU que en un CPU, también es lógico que estas operaciones no pueden resultar tan intensivas como las que le delegaríamos a un CPU. Esto es debido a que los GPU buscan tener más procesadores más pequeños y simples. Asimismo, existen otras limitantes que generan los siguientes tres fundamentos para la programación de un GPU:\n",
        "\n",
        "\n",
        "\n",
        "*   Como hay muchas unidades de cómputo, se cambia una facilidad de control por más cómputo.\n",
        "*   Se debe seguir un modelo de programación paralelo.\n",
        "*   Se optimiza para volumen de cómputo, no latencia.\n",
        "\n",
        "Cabe mencionar que el modelo de computadora que se utiliza con una combinación de CPU y GPU es denominado heterogéneo. En este modelo, existen dos roles, el de anfitrión y el de dispositivo.\n",
        "\n",
        "Ambos tienen su propia memoria y capacidades, por lo que el sincronizar y delegar sus trabajos correctamente es vital para poder sacarles provecho.\n",
        "\n",
        "## 1.3 ¿Qué es CUDA?\n",
        "\n",
        "CUDA (Compute Unified Device Architecture) es una plataforma de computo paralelo desarrollada por Nvidia para sus GPUs. Esta permite utilizar este hardware para secciones de cómputo altamente paralelizables.\n",
        "\n",
        "El origen de CUDA se remonta al 2006, cuando se integran conceptos desarrollados previamente para extender C y darle capacidades de paralelismo.\n",
        "\n",
        "## 1.4 ¿Por qué CUDA?\n",
        "\n",
        "Por el dominio de mercado de Nvidia, la plataforma CUDA es la más utilizada en aplicaciones diversas como Deep Learning.\n",
        "\n",
        "Aunque existen competidores, no tienen la misma cantidad de soporte, ni el mismo desempeño en GPUs de Nvidia.\n",
        "\n",
        "La implementación de CUDA resulta ventajosa debido a que simplifica la programación en un GPU. Esto lo logra debido a dos factores:\n",
        "\n",
        "\n",
        "\n",
        "*   La facilidad de compartir memoria entre el anfitrión y el dispositivo. Esto debido a que CUDA provee de operaciones de traslado de memoria sencillo entre ambos dispositivos.\n",
        "*   El uso de las funciones kernel. Un kernel es la sección paralela del código, y CUDA tiene la facilidad de que se implementa como cualquier otra función. Sin embargo, debe de realizarse una asignación cuidadosa del dominio de trabajo de cada kernel individual ya que por defecto un GPU posee una arquitectura SIMD.\n",
        "\n",
        "## 1.5 Desventajas de CUDA\n",
        "\n",
        "\n",
        "\n",
        "*   Limitado únicamente a equipos Nvidia.\n",
        "*   Existe un cuello de botella generado por el bus de datos entre el GPU y el CPU.\n",
        "*   No es tan popular.\n",
        "\n",
        "\n",
        "## 1.6 Ventajas de CUDA\n",
        "\n",
        "*   Un GPU tiene un nivel de paralelización varias magnitudes más alto que un CPU, lo que deriva en una gran capacidad de resolver problemas paralelizables, como por ejemplo, las operaciones tensoriales. En CUDA, la implementación para resolver este tipo de problemas es muy sencilla.\n",
        "*   Tiene soporte para escalabilidad. Es decir, puedes tener varias GPUs trabajando en paralelo sin muchos problemas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xggwNRzpL6H",
        "colab_type": "text"
      },
      "source": [
        "# 2. PyCUDA\n",
        "\n",
        "Como ya hemos visto, las GPU Nvidia utilizan CUDA para realizar cálculos en paralelo. Sin embargo, la implementación original de CUDA es en lenguaje C. Si queremos usar CUDA en otros lenguajes, como por ejemplo Python, debemos acercarnos a las implementaciones de CUDA para este mismo. Aquí entra PyCUDA, que es un wrapper para CUDA en Python.\n",
        "PyCUDA nos da toda la funcionalidad de CUDA implementada en forma de funciones, y nos da una opción de ejecutar el código C de CUDA en el mismo, por lo que resulta muy versátil para la programación con CUDA y Python, es decir. PyCUDA nos da una manera fácil de usar CUDA desde la comodidad de Python.\n",
        "\n",
        "## 2.1 ¿Por qué PyCUDA?\n",
        "\n",
        "PyCUDA tiene varias ventajas\n",
        "\n",
        "\n",
        "\n",
        "* Conveniencia. Abstracciones como pycuda.compiler.SourceModule y pycuda.gpuarray.GPUArray hacen que la programación en CUDA sea aún más conveniente que con el tiempo de ejecución basado en C de Nvidia.\n",
        "* Comprobación automática de errores. Todos los errores de CUDA se traducen automáticamente en excepciones de Python.\n",
        "* Velocidad. La capa base de PyCUDA está escrita en C++, por lo que a pesar de que hay un sacrificio en el rendimiento a cambio de la comodidad de Python éste no es dramático.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUd4W6ZKG1Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pycuda.gpuarray as gpuarray\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaBpr5m4HoMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_gpu = gpuarray.to_gpu(np.random.randn(4,4).astype(np.float32))\n",
        "\n",
        "a_doubled = (2*a_gpu).get()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwrzLpaTiUIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convertimos el arreglo a float32 por que las tarjetas gráficas de Nvidia suelen trabajar con solo un grado de precisión\n",
        "a = np.random.randn(4).astype(np.float32)\n",
        "\n",
        "# Obtenemos la cantidad de memoria, necesaria para alojar el arreglo de numpy\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\n",
        "\n",
        "############################\n",
        "\n",
        "# Convertimos el arreglo a float32 por que las tarjetas gráficas de Nvidia suelen trabajar con solo un grado de precisión\n",
        "\n",
        "b = np.random.randn(4).astype(np.float32)\n",
        "\n",
        "\n",
        "# Obtenemos la cantidad de memoria, necesaria para alojar el arreglo de numpy\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "\n",
        "# Alojamos el arreglo en la localidad de memoria almacenada\n",
        "cuda.memcpy_htod(b_gpu, b)\n",
        "\n",
        "############################\n",
        "\n",
        "# Convertimos el arreglo a float32 por que las tarjetas gráficas de Nvidia suelen trabajar con solo un grado de precisión\n",
        "\n",
        "c = np.random.randn(4).astype(np.float32)\n",
        "\n",
        "\n",
        "# Obtenemos la cantidad de memoria, necesaria para alojar el arreglo de numpy\n",
        "c_gpu = cuda.mem_alloc(c.nbytes)\n",
        "\n",
        "# Alojamos el arreglo en la localidad de memoria almacenada\n",
        "cuda.memcpy_htod(c_gpu, c)\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "  __global__ void doublify(float *a)\n",
        "  {\n",
        "    int idx = threadIdx.x + threadIdx.y*4;\n",
        "    a[idx] *= 2;\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "mod_a = SourceModule(\"\"\"\n",
        "  __global__ void VecAdd(float* A, float* B, float* C)\n",
        "  {\n",
        "      int i = threadIdx.x;\n",
        "      C[i] = A[i] + B[i];\n",
        "  }\n",
        "\"\"\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfXVHjzKOMli",
        "colab_type": "code",
        "outputId": "8548a0f7-cdff-4766-c783-1bff8353c638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "func = mod.get_function(\"doublify\")\n",
        "func2 = mod_a.get_function(\"VecAdd\")\n",
        "\n",
        "func(a_gpu, block=(4,4,1))\n",
        "\n",
        "type(func)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pycuda._driver.Function"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg1S_uJ_8j5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func = mod.get_function(\"doublify\")\n",
        "func2 = mod_a.get_function(\"VecAdd\")\n",
        "\n",
        "func(a_gpu, block=(4,4,1))\n",
        "\n",
        "func2(a_gpu, b_gpu, c_gpu, block=(4,4,1))\n",
        "\n",
        "a_doubled = np.empty_like(a)\n",
        "b_doubled = np.empty_like(b)\n",
        "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
        "cuda.memcpy_dtoh(b_doubled, c_gpu)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXL9wcf98Jhc",
        "colab_type": "code",
        "outputId": "9c1f8e09-36fa-462d-95e0-3c6eaf8cd289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(b_doubled)\n",
        "print(a, b, c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.1595837 -1.9011077 -2.2535782 15.994909 ]\n",
            "[ 0.75400907 -0.45637074  1.3098347   0.386344  ] [-1.3792673   0.13927236  1.8463005   0.7482839 ] [-0.07656808  1.310187    0.30812946  1.0180608 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3obEqheS_v7",
        "colab_type": "text"
      },
      "source": [
        "### Ejemplo: multiplicación de matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCQaDRDnTEXG",
        "colab_type": "code",
        "outputId": "0ecd11ad-e726-491e-cf72-3247801c9a89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import pycuda.gpuarray as gpuarray\n",
        "import pycuda.autoinit\n",
        "import numpy as np\n",
        "import skcuda.linalg\n",
        "import time\n",
        "\n",
        "# --- Initializations\n",
        "skcuda.linalg.init()\n",
        "\n",
        "A = np.random.random((10,10)).astype(np.float64)\n",
        "B = np.random.random((10,10)).astype(np.float64)\n",
        "\n",
        "#t = time.time()\n",
        "\n",
        "A_gpu = gpuarray.to_gpu(A)\n",
        "B_gpu = gpuarray.to_gpu(B)\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "C_gpu = skcuda.linalg.dot(A_gpu, B_gpu)\n",
        "\n",
        "print(\"Tiempo en paralelo: {} s\".format(time.time()-t))\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "np.dot(A, B)\n",
        "\n",
        "print(\"Tiempo secuencial: {} s\".format(time.time()-t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tiempo en paralelo: 0.006435394287109375 s\n",
            "Tiempo secuencial: 0.00015783309936523438 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc_0KKRkXjN-",
        "colab_type": "text"
      },
      "source": [
        "En lo anterior, vemos que es más ráṕido de manera secuencial, pero eso era con una matriz pequeña, ahora tratemos con una matriz más grande"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0xuvpMgWiGY",
        "colab_type": "code",
        "outputId": "c03c5d15-6e7a-403e-c4c9-69e5b2e4cdd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# --- Initializations\n",
        "skcuda.linalg.init()\n",
        "\n",
        "A = np.random.random((5000,5000)).astype(np.float64)\n",
        "B = np.random.random((5000,5000)).astype(np.float64)\n",
        "\n",
        "#t = time.time()\n",
        "\n",
        "A_gpu = gpuarray.to_gpu(A)\n",
        "B_gpu = gpuarray.to_gpu(B)\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "C_gpu = skcuda.linalg.dot(A_gpu, B_gpu)\n",
        "\n",
        "print(\"Tiempo en paralelo: {} s\".format(time.time()-t))\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "np.dot(A, B)\n",
        "\n",
        "print(\"Tiempo secuencial: {} s\".format(time.time()-t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tiempo en paralelo: 0.06437253952026367 s\n",
            "Tiempo secuencial: 6.9050023555755615 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5ftpKiNdkg4",
        "colab_type": "text"
      },
      "source": [
        "### Ejemplo: integrales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyzoh46ednnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pycuda.autoinit\n",
        "import pycuda.gpuarray\n",
        "import numpy as np\n",
        "import skcuda.integrate\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRiOGVYMdqwQ",
        "colab_type": "code",
        "outputId": "106d59c4-c7d2-4a94-900c-dc2fb8fa7984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dx = 0.001\n",
        "\n",
        "x = np.arange(0, np.pi, dx, dtype=np.float32) \n",
        "y = np.array(np.sin(x**2 + np.cos(x)), dtype=np.float32)\n",
        "\n",
        "y_gpu = gpuarray.to_gpu(y)\n",
        "\n",
        "# Empezamos la parte en paralelo\n",
        "skcuda.integrate.init()\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "skcuda.integrate.trapz(y_gpu, dx=dx)\n",
        "\n",
        "print(\"Tiempo en paralelo: {} s\".format(time.time()-t))\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "scipy.trapz(y, x)\n",
        "\n",
        "print(\"Tiempo secuencial: {} s\".format(time.time()-t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tiempo en paralelo: 0.10836362838745117 s\n",
            "Tiempo secuencial: 0.0008156299591064453 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhrxRboxeWWN",
        "colab_type": "code",
        "outputId": "bdf272af-24ea-4a00-9a69-00c35306df1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dx = 0.001\n",
        "dy = 0.001\n",
        "\n",
        "xs = np.arange(0, np.pi, dx, dtype=np.float32) \n",
        "ys = np.arange(0, np.pi, dy, dtype=np.float32)\n",
        "\n",
        "z = np.array([[np.sin(x**2 + np.cos(y)) for x in xs] for y in ys])\n",
        "\n",
        "#t = time.time()\n",
        "\n",
        "# Mandamos nuestra matriz a la GPU, pero si la matriz es grande la comunicación toma un tiempo considerable\n",
        "z_gpu = gpuarray.to_gpu(z)\n",
        "\n",
        "# Empezamos la parte en paralelo\n",
        "skcuda.integrate.init()\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "skcuda.integrate.trapz2d(z_gpu, dx=dx, dy=dy)\n",
        "\n",
        "print(\"Tiempo en paralelo: {} s\".format(time.time()-t))\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "# Integramos primero sobre x y luego sobre y\n",
        "scipy.trapz([scipy.trapz(z_x, xs) for z_x in z], ys)\n",
        "\n",
        "print(\"Tiempo en secuencial: {} s\".format(time.time()-t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tiempo en paralelo: 0.10402393341064453 s\n",
            "Tiempo en secuencial: 0.08076310157775879 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JljVMKfXnLcG",
        "colab_type": "text"
      },
      "source": [
        "### Ejemplo: inversa de una matriz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjk2Aht0nK2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pycuda.gpuarray as gpuarray\n",
        "import pycuda.autoinit\n",
        "import numpy as np\n",
        "import skcuda.linalg\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_xHnOPgnhvj",
        "colab_type": "code",
        "outputId": "31098eb6-c153-498c-bc50-8785abc3bddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "A = np.random.random((5000,5000)).astype(np.float64)\n",
        "\n",
        "#t = time.time()\n",
        "\n",
        "A_gpu = gpuarray.to_gpu(A)\n",
        "\n",
        "# Empezamos la parte en paralelo\n",
        "skcuda.linalg.init()\n",
        "\n",
        "#t = time.time()\n",
        "\n",
        "skcuda.linalg.inv(A_gpu)\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "print(\"Tiempo en paralelo: {} s\".format(time.time()-t))\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "np.linalg.inv(A)\n",
        "\n",
        "print(\"Tiempo en secuencial: {} s\".format(time.time()-t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tiempo en paralelo: 5.888938903808594e-05 s\n",
            "Tiempo en secuencial: 10.10940408706665 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnqkwpiwZ-Gk",
        "colab_type": "text"
      },
      "source": [
        "### Ejemplo: red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I2Xc9aLYyjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import skcuda.linalg as linalg\n",
        "import skcuda.misc as misc\n",
        "import time\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaYk-ITic8N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapaGPU:  \n",
        "  L=False  \n",
        "  \n",
        "  #Funciones de Activación\n",
        "  def sigmoide(self,z):\n",
        "    return 1 / (1+math.e**(-1*z))\n",
        "  def tanh(self,z):\n",
        "    return (math.e**(z)-math.e**(-1*z))/(math.e**(z)+math.e**(-1*z))\n",
        "\n",
        "  #Derivadas de las Funciones de Activación\n",
        "  def sigmoidep(self,z):\n",
        "    a=self.sigmoide(z)\n",
        "    return a*(1-a)\n",
        "  def tanhp (self,z):\n",
        "    a=self.tanh(z)\n",
        "    return 1-a**2\n",
        "\n",
        "    \n",
        "  def Mul(self,matZ,matY):\n",
        "    Res=matZ.copy()\n",
        "\n",
        "    for i in range(matZ.shape[0]):\n",
        "      for j in range(matZ.shape[-1]):\n",
        "        Res[i,j]=matZ[i,j]*matY[i,j]\n",
        "    # linalg.dot(gpuarray.to_gpu(matZ), gpuarray.to_gpu(matY)).get() \n",
        "    return Res\n",
        "  \n",
        "  def __init__(self,nl_1,nl,act='sigmoide'):\n",
        "    self.W=np.asmatrix(np.random.randn(nl,nl_1))\n",
        "    self.b=np.asmatrix(np.random.rand(nl,1))\n",
        "\n",
        "    if act == 'sigmoide':\n",
        "      self.g=self.sigmoide\n",
        "      self.gp=self.sigmoidep\n",
        "    elif act == 'tanh':\n",
        "      self.g=self.tanh\n",
        "      self.gp=self.tanhp\n",
        "    elif act == 'relu':\n",
        "      self.g = self.relu\n",
        "      self.gp=self.relup\n",
        "    elif act == 'Lrelu':\n",
        "      self.g = self.Lrelu\n",
        "      self.gp = self.Lrelup\n",
        "  \n",
        "  def Activar(self,matX):\n",
        "    \n",
        "    self.Z = self.W*matX + self.b\n",
        "    # self.W = np.ascontiguousarray(self.W)\n",
        "    # matX = np.ascontiguousarray(matX)\n",
        "    # self.Z = misc.multiply(gpuarray.to_gpu(self.W), gpuarray.to_gpu(matX)).get() + self.b\n",
        "    self.A = self.Z.copy()\n",
        "    self.Zp = self.Z.copy()\n",
        "    for i in range(self.Z.shape[0]):\n",
        "      for j in range(self.Z.shape[-1]):\n",
        "        self.A[i,j]=self.g(self.Z[i,j])\n",
        "        self.Zp[i,j]=self.gp(self.Z[i,j])\n",
        "        \n",
        "  def Errores(self,matY,matAl_1,Wn1=None,dZn1=None):\n",
        "    if(self.L):\n",
        "      self.dA = self.A - matY\n",
        "    else:\n",
        "      self.dA = Wn1.T * dZn1\n",
        "      # self.dA = misc.multiply(gpuarray.to_gpu(Wn1.T), gpuarray.to_gpu(dZn1)).get()\n",
        "    self.dZ = self.Mul(self.dA,self.Zp)\n",
        "    self.dW = (1 / self.Z.shape[0] )*self.dZ * matAl_1.T\n",
        "    self.db = (1/self.Z.shape[0])*np.sum(self.dZ,axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svFJ6SxEdDC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Definición de la clase Red:\n",
        "Esta contendrá todas las Capas de la red\n",
        "Además, tendrá funciones para activar la red,\n",
        "entrenarla, Backpropagation, Predecir, el discriminador\n",
        "y una función para separar los datos en los conjuntos de \n",
        "entrenamiento, desarrollo y validación.\n",
        "\"\"\"\n",
        "class RedNeuronalGPU:\n",
        " \n",
        "  def __init__(self):\n",
        "    self.capas=[]\n",
        "    self.errores=[]\n",
        "  def AniadirCapa(self,capa):\n",
        "    self.capas.append(capa)\n",
        "    \n",
        "  def ActRed(self,matX):\n",
        "    if(len(self.capas)!=0):\n",
        "      self.capas[0].Activar(matX)\n",
        "      for i in range(1,len(self.capas)):\n",
        "       \n",
        "        self.capas[i].Activar(self.capas[i-1].A)\n",
        "      self.capas[-1].L=True\n",
        "    else:\n",
        "      print(\"Error! La red no tiene capas\")\n",
        "      \n",
        "  def BackPropagation(self,matX,matY):\n",
        "    if len(self.capas)==1:\n",
        "      self.capas[-1].Errores(matY,matX)\n",
        "    else:\n",
        "      self.capas[-1].Errores(matY,self.capas[-2].A)\n",
        "      for i in reversed(range(1,len(self.capas)-1)):\n",
        "        self.capas[i].Errores(matY,self.capas[i-1].A,self.capas[i+1].W,self.capas[i+1].dZ)\n",
        "      self.capas[0].Errores(matY,matX,self.capas[1].W,self.capas[1].dZ)\n",
        "              \n",
        "  def Entrenar(self,matX,matY,iteraciones=5000,alpha=0.1,lam=0.0):\n",
        "    if(len(self.capas)!=0):\n",
        "      for i in range(iteraciones):\n",
        "        self.ActRed(matX)\n",
        "        self.errores.append(1/matX.shape[-1] * np.sum((self.capas[-1].A-matY)*(self.capas[-1].A-matY).T))\n",
        "        self.BackPropagation(matX,matY)\n",
        "        for j in range(len(self.capas)):\n",
        "          self.capas[j].W = self.capas[j].W - alpha*self.capas[j].dW + lam*self.capas[j].W.sum()\n",
        "          self.capas[j].b = self.capas[j].b - alpha*self.capas[j].db + lam*self.capas[j].b.sum()\n",
        "    else:\n",
        "      print(\"Error! La red no tiene capas\")\n",
        "      \n",
        "  def Discriminador(self,matZ):\n",
        "    Res = matZ.copy()\n",
        "    for i in range(matZ.shape[0]):\n",
        "      for j in range(matZ.shape[-1]):\n",
        "        if(i==np.argmax(matZ[:,j])):\n",
        "          Res[i,j]=1\n",
        "        else:\n",
        "          Res[i,j]=0\n",
        "    return Res  \n",
        "  \n",
        "  def Predict(self,matX):\n",
        "    self.ActRed(matX)\n",
        "    return self.Discriminador(self.capas[-1].A)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNj96lf6ZDTN",
        "colab_type": "code",
        "outputId": "2a4faf22-ed3b-4726-d2d6-4b3d7ef78343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  \n",
        "y = iris.target\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OflATl6SY-_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creamos nuestra red 1 \n",
        "redGPU = RedNeuronalGPU()\n",
        "redGPU.AniadirCapa(CapaGPU(4,100,'sigmoide'))\n",
        "redGPU.AniadirCapa(CapaGPU(100,100,'tanh'))\n",
        "redGPU.AniadirCapa(CapaGPU(100,3,'sigmoide'))\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "redGPU.Entrenar(X.T, y, 3000)\n",
        "\n",
        "print(\"Tiempo en paralelo: {} s\".format(time.time()-t1))\n",
        "\n",
        "#Vemos el error respecto a las iteraciones\n",
        "plt.plot(np.arange(len(redGPU.errores)),redGPU.errores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wszvTvyIms72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arreglo = gpuarray.to_gpu(redGPU.capas[0].W.T).astype('float32')\n",
        "arreglo1 = gpuarray.to_gpu(redGPU.capas[1].W.T).astype('float32')\n",
        "arreglo2 = gpuarray.to_gpu(redGPU.capas[2].W.T).astype('float32')\n",
        "\n",
        "def sig(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def predecir(x):\n",
        "  x = gpuarray.to_gpu(x).astype('float32')\n",
        "  z1 = linalg.dot(x, arreglo).get() + redGPU.capas[0].b.T\n",
        "  a1 = gpuarray.to_gpu(sig(z1)).astype('float32')\n",
        "  z2 = linalg.dot(a1, arreglo1).get() + redGPU.capas[1].b.T\n",
        "  a2 = gpuarray.to_gpu(sig(z2)).astype('float32')\n",
        "  zL = linalg.dot(a2, arreglo2).get() \n",
        "  return sig(zL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qhb4PULhfmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "suma = 0\n",
        "for i in range(150):\n",
        "  ejemplo = X[i]\n",
        "  t1 = time.time()\n",
        "\n",
        "  ale = np.argmax(redGPU.Predict(ejemplo.reshape((len(ejemplo), 1))))\n",
        "  print(\"Tiempo en paralelo: {} s\".format(time.time()-t1))\n",
        "  suma += 1 if ale == y[i] else 0\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px05IzjOhgGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "suma = 0\n",
        "for i in range(150):\n",
        "  ejemplo = X[i]\n",
        "  t1 = time.time()\n",
        "  ale = np.argmax(predecir(ejemplo))\n",
        "  print(\"Tiempo en paralelo: {} s\".format(time.time()-t1))\n",
        "  suma += 1 if ale == y[i] else 0\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBSJOEu7pTJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "suma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tosl9hMxpwTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AuUxOrwtcO0",
        "colab_type": "text"
      },
      "source": [
        "# Referencias útiles\n",
        "\n",
        "\n",
        "\n",
        "1.   Para checar las funciones de parte de scikit-cuda ver [scikit-cuda](https://scikit-cuda.readthedocs.io/en/latest/reference.html).\n",
        "2.   Para ver los avances de Nvidia con CUDA checar [Nvidia CUDA Zone](https://developer.nvidia.com/cuda-zone).\n",
        "3. Algunas aplicaciones en las que se usa CUDA [CUDA Technology](https://www.geforce.com/hardware/technology/cuda).\n",
        "4. Documentación de CUDA [CUDA Toolkit Documentation](https://docs.nvidia.com/cuda/).\n",
        "5. Documentación de PyCUDA [PyCUDA Documentation](https://documen.tician.de/pycuda/)\n",
        "6. Muy breve introduccióón a PyCUDA [PyCUDA Intro](http://www.iitk.ac.in/hpc4se/downloads/PyCUDA-AH.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KADbJqZDt7K7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}